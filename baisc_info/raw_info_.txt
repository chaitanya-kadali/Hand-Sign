total code base : 

!python -m pip install numpy==1.26.4

!pip install opencv-python==4.7.0.68


pip install --user mediapipe==0.10.5
pip install scikit-learn==1.2.0














python collect_imgs.py  {

import os

import cv2


DATA_DIR = './data'
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# num of input symbols u wanna take at each run time
number_of_classes = 1  
dataset_size = 100

cap = cv2.VideoCapture(0)
# the value at the above function to be changed from 0 to 1,2,3 based on the brand of computer
for j in range(number_of_classes):
    if not os.path.exists(os.path.join(DATA_DIR, str(j))):
        os.makedirs(os.path.join(DATA_DIR, str(j)))

    print('Collecting data for class {}'.format(j))

    done = False
    while True:
        ret, frame = cap.read()
        cv2.putText(frame, 'Ready? Press "Q" ! :)', (100, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3,
                    cv2.LINE_AA)
        cv2.imshow('frame', frame)
        if cv2.waitKey(25) == ord('q'):
            break

    counter = 0
    while counter < dataset_size:
        ret, frame = cap.read()
        cv2.imshow('frame', frame)
        cv2.waitKey(25)
        cv2.imwrite(os.path.join(DATA_DIR, str(j), '{}.jpg'.format(counter)), frame)

        counter += 1

cap.release()
cv2.destroyAllWindows()


}

pip install matplotlib

pip install pickle-mixin

python create_dataset.py{

import os
import pickle

import mediapipe as mp
import cv2
import matplotlib.pyplot as plt


mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)

DATA_DIR = './data'

data = []
labels = []
for dir_ in os.listdir(DATA_DIR):
    print("Running so wait\n")
    dir_path = os.path.join(DATA_DIR, dir_)
    if not os.path.isdir(dir_path):
        continue

    img_list = os.listdir(dir_path) 
    for img_path in img_list:
        data_aux = []

        x_ = []
        y_ = []

        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        results = hands.process(img_rgb)
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                for i in range(len(hand_landmarks.landmark)):
                    x = hand_landmarks.landmark[i].x
                    y = hand_landmarks.landmark[i].y

                    x_.append(x)
                    y_.append(y)

                for i in range(len(hand_landmarks.landmark)):
                    x = hand_landmarks.landmark[i].x
                    y = hand_landmarks.landmark[i].y
                    data_aux.append(x - min(x_))
                    data_aux.append(y - min(y_))

            data.append(data_aux)
            labels.append(dir_)

f = open('data.pickle', 'wb')
pickle.dump({'data': data, 'labels': labels}, f)
f.close()


}

python train_classifier.py{

import pickle

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np


data_dict = pickle.load(open('./data.pickle', 'rb'))

data = np.asarray(data_dict['data'])
labels = np.asarray(data_dict['labels'])

x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)

model = RandomForestClassifier()

model.fit(x_train, y_train)

y_predict = model.predict(x_test)

score = accuracy_score(y_predict, y_test)

print('{}% of samples were classified correctly !'.format(score * 100))

f = open('model.p', 'wb')
pickle.dump({'model': model}, f)
f.close()


}

python inference_classifier.py{

import pickle

import cv2
import mediapipe as mp
import numpy as np

model_dict = pickle.load(open('./model.p', 'rb'))
model = model_dict['model']

cap = cv2.VideoCapture(0)
# this 0 for above... needs to be changed and depends brand of company
# each laptop brand have each code, but most of them have 0 

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)

labels_dict = {0: 'Thank You !', 1: 'Welcome', 2: 'Yes', 3:'No', 4:'Hello'}
while True:

    data_aux = []
    x_ = []
    y_ = []

    ret, frame = cap.read()

    H, W, _ = frame.shape

    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    results = hands.process(frame_rgb)
    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            mp_drawing.draw_landmarks(
                frame,  # image to draw
                hand_landmarks,  # model output
                mp_hands.HAND_CONNECTIONS,  # hand connections
                mp_drawing_styles.get_default_hand_landmarks_style(),
                mp_drawing_styles.get_default_hand_connections_style())

        for hand_landmarks in results.multi_hand_landmarks:
            for i in range(len(hand_landmarks.landmark)):
                x = hand_landmarks.landmark[i].x
                y = hand_landmarks.landmark[i].y

                x_.append(x)
                y_.append(y)

            for i in range(len(hand_landmarks.landmark)):
                x = hand_landmarks.landmark[i].x
                y = hand_landmarks.landmark[i].y
                data_aux.append(x - min(x_))
                data_aux.append(y - min(y_))

        x1 = int(min(x_) * W) - 10
        y1 = int(min(y_) * H) - 10

        x2 = int(max(x_) * W) - 10
        y2 = int(max(y_) * H) - 10

        prediction = model.predict([np.asarray(data_aux)])

        predicted_character = labels_dict[int(prediction[0])]

        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)
        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3,
                    cv2.LINE_AA)

    cv2.imshow('frame', frame)
    cv2.waitKey(1)


cap.release()
cv2.destroyAllWindows()


}
























































remaining 3 modals which performed following accuracy : 


for first modal SFIT :
```
## Line graph for model 1 (SIFT)

import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
import glob

# 1. Load Images (both .jpg and .jpeg)
image_paths = glob.glob("images/*.jpg") + glob.glob("images/*.jpeg")
images = [cv2.imread(img, 0) for img in image_paths]  # Load as grayscale

# Check if images are loaded
if len(images) < 15:
    print(f"Warning: Found only {len(images)} images. Ensure you have 13 training + 2 testing images.")
    exit()

# 2. Define Training & Testing Data
train_images = images[:13]  # First 13 images as training
test_images = images[13:]   # Last 2 images as testing

# 3. Feature Extraction using SIFT
sift = cv2.SIFT_create()

def extract_features(image_list):
    feature_vectors = []
    for img in image_list:
        keypoints, descriptors = sift.detectAndCompute(img, None)
        if descriptors is not None:
            feature_vectors.append(descriptors.flatten())
    return feature_vectors

train_features = extract_features(train_images)
test_features = extract_features(test_images)

# Handle varying feature sizes by padding to the max length
max_length = max(max(len(fv) for fv in train_features), max(len(fv) for fv in test_features))
train_features = np.array([np.pad(fv, (0, max_length - len(fv)), 'constant') for fv in train_features])
test_features = np.array([np.pad(fv, (0, max_length - len(fv)), 'constant') for fv in test_features])

# 4. Compute Similarity between each test image and all training images
similarity_scores = cosine_similarity(test_features, train_features)

# 5. Plot the Similarity Graph
plt.figure(figsize=(10, 6))

# Plot similarity for each test image
for i, test_score in enumerate(similarity_scores):
    plt.plot(range(1, 14), test_score, marker='o', linestyle='-', label=f"Test Image {i+1}")

# Add labels and formatting
plt.title("Hand Gesture Similarity (Training vs Testing)")
plt.xlabel("Training Image Index")
plt.ylabel("Similarity Score")
plt.xticks(range(1, 14))  # Ensure X-axis labels match training images
plt.ylim(0, 1)  # Similarity score range
plt.legend()
plt.grid(True)

plt.show()





print("---\n\n\n")



## Coloured graph for model 1 (SIFT)


# 1. Load Images (both .jpg and .jpeg)
image_paths = glob.glob("images/*.jpg") + glob.glob("images/*.jpeg")
images = [cv2.imread(img, 0) for img in image_paths]  # Load as grayscale

# Check if images are loaded
if len(images) < 15:
    print(f"Warning: Found only {len(images)} images. Ensure you have 13 training + 2 testing images.")
    exit()

# 2. Define Training & Testing Data
train_images = images[:13]  # First 13 images as training
test_images = images[13:]   # Last 2 images as testing

# 3. Feature Extraction using SIFT
sift = cv2.SIFT_create()

def extract_features(image_list):
    feature_vectors = []
    for img in image_list:
        keypoints, descriptors = sift.detectAndCompute(img, None)
        if descriptors is not None:
            feature_vectors.append(descriptors.flatten())
    return feature_vectors

train_features = extract_features(train_images)
test_features = extract_features(test_images)

# Handle varying feature sizes by padding to the max length
max_length = max(max(len(fv) for fv in train_features), max(len(fv) for fv in test_features))
train_features = np.array([np.pad(fv, (0, max_length - len(fv)), 'constant') for fv in train_features])
test_features = np.array([np.pad(fv, (0, max_length - len(fv)), 'constant') for fv in test_features])

# 4. Compute Similarity between each test image and all training images
similarity_scores = cosine_similarity(test_features, train_features)

# 5. Plot the Similarity Graph with "Viridis" Color Gradient
plt.figure(figsize=(10, 6))
cmap = plt.cm.viridis

for i, test_score in enumerate(similarity_scores):
    for j, score in enumerate(test_score):
        plt.plot([j+1, j+2], [score, score], color=cmap(score), linewidth=2)  # Color depends on similarity score
    
    plt.scatter(range(1, 14), test_score, marker='o', label=f"Test Image {i+1}", c=[test_score], cmap="viridis", edgecolors='black', s=100)

# Add labels and formatting
plt.title("Hand Gesture Similarity (Training vs Testing) using SIFT")
plt.xlabel("Training Image Index")
plt.ylabel("Similarity Score")
plt.xticks(range(1, 14))
plt.ylim(0, 1)
plt.colorbar(label="Similarity Score")
plt.legend()
plt.grid(True, linestyle="--", alpha=0.7)
plt.show()







print("---\n\n\n")






# accuracy table for model 1


from sklearn.metrics import accuracy_score, precision_score, classification_report
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics.pairwise import cosine_similarity

# 1. Load Images (both .jpg and .jpeg)
image_paths = glob.glob("images/*.jpg") + glob.glob("images/*.jpeg")
images = [cv2.imread(img, 0) for img in image_paths]  # Load as grayscale

# Check if images are loaded
if len(images) < 15:
    print(f"Warning: Found only {len(images)} images. Ensure you have 13 training + 2 testing images.")
    exit()

# 2. Define Training & Testing Data
train_images = images[:13]  # First 13 images as training
test_images = images[13:]   # Last 2 images as testing

# 3. Feature Extraction using SIFT
sift = cv2.SIFT_create()

def extract_mean_features(image_list):
    feature_vectors = []
    for img in image_list:
        keypoints, descriptors = sift.detectAndCompute(img, None)
        if descriptors is not None:
            # Compute mean of descriptors to get a fixed-size feature representation
            mean_descriptor = np.mean(descriptors, axis=0)
        else:
            mean_descriptor = np.zeros(128)  # SIFT descriptors are 128-dimensional
        feature_vectors.append(mean_descriptor)
    return np.array(feature_vectors)

train_features = extract_mean_features(train_images)
test_features = extract_mean_features(test_images)

# 4. Compute Similarity between each test image and all training images
similarity_scores = cosine_similarity(test_features, train_features)

# 5. Compute Evaluation Metrics
# Instead of assuming all images should be identical, define a threshold
threshold = 0.5
predicted_labels = (similarity_scores >= threshold).astype(int)
true_labels = np.ones_like(predicted_labels)  # Assuming similarity exists

# Compute Metrics
accuracy = accuracy_score(true_labels.flatten(), predicted_labels.flatten())
precision = precision_score(true_labels.flatten(), predicted_labels.flatten(), zero_division=1)
mae = mean_absolute_error(true_labels, similarity_scores)
mse = mean_squared_error(true_labels, similarity_scores)
rmse = np.sqrt(mse)
r2 = r2_score(true_labels, similarity_scores)

# Print Output in Requested Format
print(f"SIFT Accuracy: {accuracy * 100:.2f}%")
print(f"SIFT Precision: {precision:.4f}\n")
print("Classification Report:\n", classification_report(true_labels.flatten(), predicted_labels.flatten()))
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")



output : 

SIFT Accuracy: 100.00%
SIFT Precision: 1.0000

Classification Report:
               precision    recall  f1-score   support

           1       1.00      1.00      1.00        26

    accuracy                           1.00        26
   macro avg       1.00      1.00      1.00        26
weighted avg       1.00      1.00      1.00        26

Mean Absolute Error (MAE): 0.0537
Mean Squared Error (MSE): 0.0036
Root Mean Squared Error (RMSE): 0.0598

```



















for second modal CSM(VGG16) Algorithm Model-2 : 

```


import glob
import cv2
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import img_to_array
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import accuracy_score, precision_score, classification_report, mean_absolute_error, mean_squared_error, r2_score

# Load VGG16 Model
base_model = VGG16(weights='imagenet', include_top=False, pooling='avg')
model = Model(inputs=base_model.input, outputs=base_model.output)

# Load Images
def load_images():
    image_paths = glob.glob("images/*.jpg") + glob.glob("images/*.jpeg")
    images = [cv2.imread(img) for img in image_paths]
    return images

images = load_images()
if len(images) < 15:
    print(f"Warning: Found only {len(images)} images. Ensure you have 13 training + 2 testing images.")
    exit()

# Train-Test Split
train_images = images[:13]
test_images = images[13:]

# Preprocess Images for VGG16
def preprocess_images(image_list):
    processed_images = []
    for img in image_list:
        img = cv2.resize(img, (224, 224))  # Resize to VGG16 input size
        img = img_to_array(img)
        img = np.expand_dims(img, axis=0)
        img = preprocess_input(img)
        processed_images.append(img)
    return np.vstack(processed_images)

train_images_processed = preprocess_images(train_images)
test_images_processed = preprocess_images(test_images)

# Extract Features using VGG16
def extract_features(image_batch):
    return model.predict(image_batch)

train_features = extract_features(train_images_processed)
test_features = extract_features(test_images_processed)

# Compute Cosine Similarity
similarity_scores = cosine_similarity(test_features, train_features)

# Plot Line Graph
plt.figure(figsize=(10, 6))
for i, test_score in enumerate(similarity_scores):
    plt.plot(range(1, 14), test_score, marker='o', linestyle='-', label=f"Test Image {i+1}")
plt.title("Hand Gesture Similarity (VGG16 Model)")
plt.xlabel("Training Image Index")
plt.ylabel("Similarity Score")
plt.xticks(range(1, 14))
plt.ylim(0, 1)
plt.legend()
plt.grid(True)
plt.show()

# Plot Colored Graph
plt.figure(figsize=(10, 6))
cmap = plt.cm.viridis
for i, test_score in enumerate(similarity_scores):
    plt.scatter(range(1, 14), test_score, marker='o', label=f"Test Image {i+1}", c=[test_score], cmap="viridis", edgecolors='black', s=100)
plt.title("Hand Gesture Similarity (VGG16 Model)")
plt.xlabel("Training Image Index")
plt.ylabel("Similarity Score")
plt.xticks(range(1, 14))
plt.ylim(0, 1)
plt.colorbar(label="Similarity Score")
plt.legend()
plt.grid(True, linestyle="--", alpha=0.7)
plt.show()

# Accuracy Metrics
threshold = 0.5
predicted_labels = (similarity_scores >= threshold).astype(int)
true_labels = np.ones_like(predicted_labels)

accuracy = accuracy_score(true_labels.flatten(), predicted_labels.flatten())
precision = precision_score(true_labels.flatten(), predicted_labels.flatten(), zero_division=1)
mae = mean_absolute_error(true_labels, similarity_scores)
mse = mean_squared_error(true_labels, similarity_scores)
rmse = np.sqrt(mse)
r2 = r2_score(true_labels, similarity_scores)

print(f"VGG16 Accuracy: {accuracy * 100:.2f}%")
print(f"VGG16 Precision: {precision:.4f}\n")
print("Classification Report:\n", classification_report(true_labels.flatten(), predicted_labels.flatten()))
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")


output : 

VGG16 Accuracy: 38.46%
VGG16 Precision: 1.0000

Classification Report:
               precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       1.00      0.38      0.56        26

    accuracy                           0.38        26
   macro avg       0.50      0.19      0.28        26
weighted avg       1.00      0.38      0.56        26

Mean Absolute Error (MAE): 0.5137
Mean Squared Error (MSE): 0.2747
Root Mean Squared Error (RMSE): 0.5241

  ```

for third modal modal RFC : 

```
import pickle
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, classification_report
from sklearn.preprocessing import LabelEncoder

# Load Data
data_dict = pickle.load(open('./data.pickle', 'rb'))

data = np.asarray(data_dict['data'])
labels = np.asarray(data_dict['labels'])

# Train-Test Split
x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)

# Train Random Forest Classifier
model = RandomForestClassifier()
model.fit(x_train, y_train)

y_predict = model.predict(x_test)

# Calculate Classification Metrics
score = accuracy_score(y_test, y_predict)
precision = precision_score(y_test, y_predict, average='weighted', zero_division=1)

print(f'Accuracy: {score * 100:.2f}%')
print(f'Precision: {precision:.4f}')
print("\nClassification Report:\n", classification_report(y_test, y_predict))

# ---- Optional: Convert Labels to Numeric for Regression Metrics ----
encoder = LabelEncoder()
y_test_encoded = encoder.fit_transform(y_test)
y_predict_encoded = encoder.transform(y_predict)

# Compute MAE and MSE only if needed
mae = np.mean(np.abs(y_test_encoded - y_predict_encoded))
mse = np.mean((y_test_encoded - y_predict_encoded) ** 2)
rmse = np.sqrt(mse)

print(f'Mean Absolute Error (MAE): {mae:.4f}')
print(f'Mean Squared Error (MSE): {mse:.4f}')
print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')

# Plot Line Graph
plt.figure(figsize=(10, 6))
plt.plot(range(len(y_test)), y_test_encoded, marker='o', linestyle='-', label='Actual Labels (Encoded)', color='blue')
plt.plot(range(len(y_predict)), y_predict_encoded, marker='s', linestyle='--', label='Predicted Labels (Encoded)', color='red')
plt.xlabel("Test Sample Index")
plt.ylabel("Encoded Label")
plt.title("Random Forest Classification Performance")
plt.legend()
plt.grid(True)
plt.show()

# Heatmap-like Visualization
plt.figure(figsize=(10, 6))
cmap = plt.cm.viridis
colors = [cmap(val / max(y_test_encoded)) for val in y_test_encoded]
plt.scatter(range(len(y_test_encoded)), y_test_encoded, c=colors, cmap="viridis", edgecolors='black', s=100, label="Actual")
plt.scatter(range(len(y_predict_encoded)), y_predict_encoded, c='red', marker='x', label="Predicted")
plt.colorbar(label="Label Scale")
plt.xlabel("Test Sample Index")
plt.ylabel("Encoded Label")
plt.title("Prediction vs Actual (Color-Coded)")
plt.legend()
plt.grid(True, linestyle="--", alpha=0.7)
plt.show()



output : 
Accuracy: 100.00%
Precision: 1.0000

Classification Report:
               precision    recall  f1-score   support

           0       1.00      1.00      1.00        20
           1       1.00      1.00      1.00        20
           2       1.00      1.00      1.00        20
           3       1.00      1.00      1.00        20
           4       1.00      1.00      1.00        20

    accuracy                           1.00       100
   macro avg       1.00      1.00      1.00       100
weighted avg       1.00      1.00      1.00       100

Mean Absolute Error (MAE): 0.0000
Mean Squared Error (MSE): 0.0000
Root Mean Squared Error (RMSE): 0.0000



  ```









